
name: Endpoint Validation Test

on:
  workflow_call:
    inputs:
      eval_function:
        description: 'Evaluation Function Name'
        required: true
        type: string
      sql_limit:
        description: 'Max number of records to fetch'
        required: false
        type: number
        default: 1000
    secrets:
      TEST_API_ENDPOINT:
        description: 'API Endpoint URL to test'
        required: false
      DB_USER:
        required: false
      DB_PASSWORD:
        required: false
      DB_HOST:
        required: false
      DB_PORT:
        required: false
      DB_NAME:
        required: false

jobs:
  run_test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          repository: 'lambda-feedback/Database-Testing'
          ref: main
          token: ${{ github.token }}
          path: Database-Testing

      - name: Set up Python Environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: pip install -r requirements.txt
        working-directory: ./Database-Testing

      - name: Run Test Script
        id: run_script
        env:
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          LOG_LEVEL: DEBUG
        working-directory: ./Database-Testing
        run: |
          python3 test_evaluation_function.py \
            --endpoint "${{ secrets.TEST_API_ENDPOINT }}" \
            --eval_function_name "${{ inputs.eval_function }}" \
            --sql_limit "${{ inputs.sql_limit }}" \
            --grade_params_json ""

          # Capture JSON output from script
          REPORT_DATA=$(cat report_data.json)
          echo "error_count=$(echo $REPORT_DATA | jq -r '.number_of_errors')" >> $GITHUB_OUTPUT
          echo "csv_filename=$(echo $REPORT_DATA | jq -r '.csv_filename')" >> $GITHUB_OUTPUT

          ERROR_COUNT=$(echo $REPORT_DATA | jq -r '.number_of_errors')
          if [ "$ERROR_COUNT" -gt 0 ]; then
            echo "::error file=test_evaluation_function_argparse.py::Test completed with $ERROR_COUNT errors."
